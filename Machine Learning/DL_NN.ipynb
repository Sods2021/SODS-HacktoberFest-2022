{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, utils, backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "!pip install shap\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential(name=\"Perceptron\", layers=[\n",
    "    layers.Dense(             #a fully connected layer\n",
    "          name=\"dense\",\n",
    "          input_dim=3,        #with 3 features as the input\n",
    "          units=1,            #and 1 node because we want 1 output\n",
    "          activation='linear' #f(x)=x\n",
    "    )\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function\n",
    "import tensorflow as tf\n",
    "def binary_step_activation(x):\n",
    "    ##return 1 if x>0 else 0 \n",
    "    return K.switch(x>0, tf.math.divide(x,x), tf.math.multiply(x,0))\n",
    "\n",
    "# build the model\n",
    "model = models.Sequential(name=\"Perceptron\", layers=[\n",
    "      layers.Dense(             \n",
    "          name=\"dense\",\n",
    "          input_dim=3,        \n",
    "          units=1,            \n",
    "          activation=binary_step_activation\n",
    "      )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 10\n",
    "model = models.Sequential(name=\"DeepNN\", layers=[\n",
    "    ### hidden layer 1\n",
    "    layers.Dense(name=\"h1\", input_dim=n_features,\n",
    "                 units=int(round((n_features+1)/2)), \n",
    "                 activation='relu'),\n",
    "    layers.Dropout(name=\"drop1\", rate=0.2),\n",
    "    \n",
    "    ### hidden layer 2\n",
    "    layers.Dense(name=\"h2\", units=int(round((n_features+1)/4)), \n",
    "                 activation='relu'),\n",
    "    layers.Dropout(name=\"drop2\", rate=0.2),\n",
    "    \n",
    "    ### layer output\n",
    "    layers.Dense(name=\"output\", units=1, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron\n",
    "inputs = layers.Input(name=\"input\", shape=(3,))\n",
    "outputs = layers.Dense(name=\"output\", units=1, \n",
    "                       activation='linear')(inputs)\n",
    "model = models.Model(inputs=inputs, outputs=outputs, \n",
    "                     name=\"Perceptron\")\n",
    "\n",
    "# DeepNN\n",
    "### layer input\n",
    "inputs = layers.Input(name=\"input\", shape=(n_features,))\n",
    "### hidden layer 1\n",
    "h1 = layers.Dense(name=\"h1\", units=int(round((n_features+1)/2)), activation='relu')(inputs)\n",
    "h1 = layers.Dropout(name=\"drop1\", rate=0.2)(h1)\n",
    "### hidden layer 2\n",
    "h2 = layers.Dense(name=\"h2\", units=int(round((n_features+1)/4)), activation='relu')(h1)\n",
    "h2 = layers.Dropout(name=\"drop2\", rate=0.2)(h2)\n",
    "### layer output\n",
    "outputs = layers.Dense(name=\"output\", units=1, activation='sigmoid')(h2)\n",
    "model = models.Model(inputs=inputs, outputs=outputs, name=\"DeepNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Extract info for each layer in a keras model.\n",
    "'''\n",
    "def utils_nn_config(model):\n",
    "    lst_layers = []\n",
    "    if \"Sequential\" in str(model): #-> Sequential doesn't show the input layer\n",
    "        layer = model.layers[0]\n",
    "        lst_layers.append({\"name\":\"input\", \"in\":int(layer.input.shape[-1]), \"neurons\":0, \n",
    "                           \"out\":int(layer.input.shape[-1]), \"activation\":None,\n",
    "                           \"params\":0, \"bias\":0})\n",
    "    for layer in model.layers:\n",
    "        try:\n",
    "            dic_layer = {\"name\":layer.name, \"in\":int(layer.input.shape[-1]), \"neurons\":layer.units, \n",
    "                         \"out\":int(layer.output.shape[-1]), \"activation\":layer.get_config()[\"activation\"],\n",
    "                         \"params\":layer.get_weights()[0], \"bias\":layer.get_weights()[1]}\n",
    "        except:\n",
    "            dic_layer = {\"name\":layer.name, \"in\":int(layer.input.shape[-1]), \"neurons\":0, \n",
    "                         \"out\":int(layer.output.shape[-1]), \"activation\":None,\n",
    "                         \"params\":0, \"bias\":0}\n",
    "        lst_layers.append(dic_layer)\n",
    "    return lst_layers\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Plot the structure of a keras neural network.\n",
    "'''\n",
    "def visualize_nn(model, description=False, figsize=(10,8)):\n",
    "    ## get layers info\n",
    "    lst_layers = utils_nn_config(model)\n",
    "    layer_sizes = [layer[\"out\"] for layer in lst_layers]\n",
    "    \n",
    "    ## fig setup\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.gca()\n",
    "    ax.set(title=model.name)\n",
    "    ax.axis('off')\n",
    "    left, right, bottom, top = 0.1, 0.9, 0.1, 0.9\n",
    "    x_space = (right-left) / float(len(layer_sizes)-1)\n",
    "    y_space = (top-bottom) / float(max(layer_sizes))\n",
    "    p = 0.025\n",
    "    \n",
    "    ## nodes\n",
    "    for i,n in enumerate(layer_sizes):\n",
    "        top_on_layer = y_space*(n-1)/2.0 + (top+bottom)/2.0\n",
    "        layer = lst_layers[i]\n",
    "        color = \"green\" if i in [0, len(layer_sizes)-1] else \"blue\"\n",
    "        color = \"red\" if (layer['neurons'] == 0) and (i > 0) else color\n",
    "        \n",
    "        ### add description\n",
    "        if (description is True):\n",
    "            d = i if i == 0 else i-0.5\n",
    "            if layer['activation'] is None:\n",
    "                plt.text(x=left+d*x_space, y=top, fontsize=10, color=color, s=layer[\"name\"].upper())\n",
    "            else:\n",
    "                plt.text(x=left+d*x_space, y=top, fontsize=10, color=color, s=layer[\"name\"].upper())\n",
    "                plt.text(x=left+d*x_space, y=top-p, fontsize=10, color=color, s=layer['activation']+\" (\")\n",
    "                plt.text(x=left+d*x_space, y=top-2*p, fontsize=10, color=color, s=\"Î£\"+str(layer['in'])+\"[X*w]+b\")\n",
    "                out = \" Y\"  if i == len(layer_sizes)-1 else \" out\"\n",
    "                plt.text(x=left+d*x_space, y=top-3*p, fontsize=10, color=color, s=\") = \"+str(layer['neurons'])+out)\n",
    "        \n",
    "        ### circles\n",
    "        for m in range(n):\n",
    "            color = \"limegreen\" if color == \"green\" else color\n",
    "            circle = plt.Circle(xy=(left+i*x_space, top_on_layer-m*y_space-4*p), radius=y_space/4.0, color=color, ec='k', zorder=4)\n",
    "            ax.add_artist(circle)\n",
    "            \n",
    "            ### add text\n",
    "            if i == 0:\n",
    "                plt.text(x=left-4*p, y=top_on_layer-m*y_space-4*p, fontsize=10, s=r'$X_{'+str(m+1)+'}$')\n",
    "            elif i == len(layer_sizes)-1:\n",
    "                plt.text(x=right+4*p, y=top_on_layer-m*y_space-4*p, fontsize=10, s=r'$y_{'+str(m+1)+'}$')\n",
    "            else:\n",
    "                plt.text(x=left+i*x_space+p, y=top_on_layer-m*y_space+(y_space/8.+0.01*y_space)-4*p, fontsize=10, s=r'$H_{'+str(m+1)+'}$')\n",
    "    \n",
    "    ## links\n",
    "    for i, (n_a, n_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        layer = lst_layers[i+1]\n",
    "        color = \"green\" if i == len(layer_sizes)-2 else \"blue\"\n",
    "        color = \"red\" if layer['neurons'] == 0 else color\n",
    "        layer_top_a = y_space*(n_a-1)/2. + (top+bottom)/2. -4*p\n",
    "        layer_top_b = y_space*(n_b-1)/2. + (top+bottom)/2. -4*p\n",
    "        for m in range(n_a):\n",
    "            for o in range(n_b):\n",
    "                line = plt.Line2D([i*x_space+left, (i+1)*x_space+left], \n",
    "                                  [layer_top_a-m*y_space, layer_top_b-o*y_space], \n",
    "                                  c=color, alpha=0.5)\n",
    "                if layer['activation'] is None:\n",
    "                    if o == m:\n",
    "                        ax.add_artist(line)\n",
    "                else:\n",
    "                    ax.add_artist(line)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_nn(model, description=True, figsize=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove('model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "def Recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def Precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def F1(y_true, y_pred):\n",
    "    precision = Precision(y_true, y_pred)\n",
    "    recall = Recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# compile the neural network\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "              metrics=['accuracy',F1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metrics\n",
    "def R2(y, y_hat):\n",
    "    ss_res =  K.sum(K.square(y - y_hat)) \n",
    "    ss_tot = K.sum(K.square(y - K.mean(y))) \n",
    "    return ( 1 - ss_res/(ss_tot + K.epsilon()) )\n",
    "\n",
    "# compile the neural network\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error', \n",
    "              metrics=[R2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.random.rand(1000,10)\n",
    "y = np.random.choice([1,0], size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/validation\n",
    "training = model.fit(x=X, y=y, batch_size=32, epochs=100, shuffle=True, verbose=0, validation_split=0.3)\n",
    "\n",
    "# plot\n",
    "metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]    \n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(15,3))\n",
    "       \n",
    "## training    \n",
    "ax[0].set(title=\"Training\")    \n",
    "ax11 = ax[0].twinx()    \n",
    "ax[0].plot(training.history['loss'], color='black')    ax[0].set_xlabel('Epochs')    \n",
    "ax[0].set_ylabel('Loss', color='black')    \n",
    "for metric in metrics:        \n",
    "    ax11.plot(training.history[metric], label=metric)    ax11.set_ylabel(\"Score\", color='steelblue')    \n",
    "ax11.legend()\n",
    "        \n",
    "## validation    \n",
    "ax[1].set(title=\"Validation\")    \n",
    "ax22 = ax[1].twinx()    \n",
    "ax[1].plot(training.history['val_loss'], color='black')    ax[1].set_xlabel('Epochs')    \n",
    "ax[1].set_ylabel('Loss', color='black')    \n",
    "for metric in metrics:          \n",
    "    ax22.plot(training.history['val_'+metric], label=metric)    ax22.set_ylabel(\"Score\", color=\"steelblue\")    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use shap to build an a explainer.\n",
    ":parameter\n",
    "    :param model: model instance (after fitting)\n",
    "    :param X_names: list\n",
    "    :param X_instance: array of size n x 1 (n,)\n",
    "    :param X_train: array - if None the model is simple machine learning, if not None then it's a deep learning model\n",
    "    :param task: string - \"classification\", \"regression\"\n",
    "    :param top: num - top features to display\n",
    ":return\n",
    "    dtf with explanations\n",
    "'''\n",
    "def explainer_shap(model, X_names, X_instance, X_train=None, task=\"classification\", top=10):\n",
    "    ## create explainer\n",
    "    ### machine learning\n",
    "    if X_train is None:\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_instance)\n",
    "    ### deep learning\n",
    "    else:\n",
    "        explainer = shap.DeepExplainer(model, data=X_train[:100])\n",
    "        shap_values = explainer.shap_values(X_instance.reshape(1,-1))[0].reshape(-1)\n",
    "\n",
    "    ## plot\n",
    "    ### classification\n",
    "    if task == \"classification\":\n",
    "        shap.decision_plot(explainer.expected_value, shap_values, link='logit', feature_order='importance',\n",
    "                           features=X_instance, feature_names=X_names, feature_display_range=slice(-1,-top-1,-1))\n",
    "    ### regression\n",
    "    else:\n",
    "        shap.waterfall_plot(explainer.expected_value[0], shap_values, \n",
    "                            features=X_instance, feature_names=X_names, max_display=top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "explainer_shap(model, \n",
    "               X_names=list_feature_names, \n",
    "               X_instance=X[i], \n",
    "               X_train=X, \n",
    "               task=\"classification\", #task=\"regression\"\n",
    "               top=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73c768b40639084f4c4534885f66a2b010198367106e87415af0b965cb13b7d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
